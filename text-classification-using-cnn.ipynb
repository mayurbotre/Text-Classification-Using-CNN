{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0750388a2837b4425e5f1dd11da60a8f9b30c7b4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "35297be0266ec89dc1786312025a26458be584d6"
   },
   "outputs": [],
   "source": [
    "# the dataset path\n",
    "TEXT_DATA_DIR = r'data'\n",
    "#the path for Glove embeddings\n",
    "GLOVE_DIR = r'embed'\n",
    "# make the max word length to be constant\n",
    "MAX_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "# the percentage of train test split to be applied\n",
    "VALIDATION_SPLIT = 0.20\n",
    "# the dimension of vectors to be used\n",
    "EMBEDDING_DIM = 100\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [2,3,4]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 50\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8f9972b8b95a38df4e08227b5a638bd675d7c945"
   },
   "outputs": [],
   "source": [
    "## preparing dataset\n",
    "\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "# print(labels_index)\n",
    "\n",
    "# print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "texts = pd.read_csv('data/yelp_1.csv')[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>apparently prides osteria had a rough summer a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this store is pretty good not as great as walm...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i called wvm on the recommendation of a couple...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ive stayed at many marriott and renaissance ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the food is always great here the service from...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1000001</td>\n",
       "      <td>this was my first time at seasons 52 and i hav...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>1000002</td>\n",
       "      <td>ive lived two doors south of here for 6 years ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>1000003</td>\n",
       "      <td>love this place i always order the guac burger...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>1000004</td>\n",
       "      <td>ill be honest i really enjoyed the laser tag i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>1000005</td>\n",
       "      <td>ive come here a few times not out of want but ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  stars\n",
       "0                0  apparently prides osteria had a rough summer a...      4\n",
       "1                1  this store is pretty good not as great as walm...      4\n",
       "2                2  i called wvm on the recommendation of a couple...      5\n",
       "3                3  ive stayed at many marriott and renaissance ma...      2\n",
       "4                4  the food is always great here the service from...      4\n",
       "...            ...                                                ...    ...\n",
       "999995     1000001  this was my first time at seasons 52 and i hav...      4\n",
       "999996     1000002  ive lived two doors south of here for 6 years ...      5\n",
       "999997     1000003  love this place i always order the guac burger...      5\n",
       "999998     1000004  ill be honest i really enjoyed the laser tag i...      4\n",
       "999999     1000005  ive come here a few times not out of want but ...      2\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels length:  1000000\n",
      "Text length:  1000000\n"
     ]
    }
   ],
   "source": [
    "labels = texts['stars']\n",
    "texts = texts['text']\n",
    "print('Labels length: ', len(labels))\n",
    "print('Text length: ', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f3058f0c6703374a384d8720712cb2151e44e8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words : 486228\n",
      "Shape of data tensor: (1000000, 1000)\n",
      "Shape of label tensor: (1000000, 6)\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences =  tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fc1c709458e9f4eb338a40c40c85dedba29c6fe8"
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0620c11d2dab62329f250ecad40bcefbf57a7134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    line = str(line)\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "07d064695cf65aaba497d6bb0dbd14dea220d533"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "74abe6ec0048d25c6169081f7cd409359588aee0"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e499119a397f180258ab0e2b8c5a6b47ef98fc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1000, 100)\n",
      "(None, 1000, 100, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1000, 100)    48622900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1000, 100, 1) 0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 999, 1, 512)  102912      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 998, 1, 512)  154112      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 997, 1, 512)  205312      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 512)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 1, 512)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1536)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1536)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           30740       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            126         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 49,116,102\n",
      "Trainable params: 493,202\n",
      "Non-trainable params: 48,622,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User01\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "print(embedding.shape)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\n",
    "print(reshape.shape)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=20, activation='softmax')(dropout)\n",
    "output = Dense(units=6, activation='softmax')(output)\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3f99fb84e45c5fdf63607020c346902c340a31a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2265/16000 [===>..........................] - ETA: 14:38 - loss: 1.5174 - accuracy: 0.4711- ETA: 16:52 - loss: 1.7279 - accuracy: 0. - ETA: 16:51 - loss: 1.7193 - accuracy: 0.3 - ETA: 16:50 - loss: 1.7131 - accuracy: 0.3 - ETA: 16:49  - ETA: 16:42 - loss: 1.6697 - accuracy: - ETA: 16:42 - loss: 1.6656 - accuracy:  - ETA: - ETA: 16:40 - loss: 1.6537 - accuracy: 0. - ETA: 16:40 - loss: 1.6536 - accuracy: 0. - ETA: 16:40 - loss: 1.6543 - accuracy: 0.42 - ETA: 16:39 - loss: 1.6544 - accurac - ETA: 16:39 - loss: 1.6507 - accuracy: 0.4 - ETA: 16:39 - loss: 1. - ETA: 16:35 - loss: 1.6449 -  - ETA:  - ETA: 16:33 - loss: 1.6409 - accuracy:  - ETA: 16:30 - loss: 1.6369 - accuracy:  - ETA: 16:30 - loss: 1.63 - ETA: 16:26 - loss: 1.6294 - accuracy: 0. - ETA: 16:26 - loss: 1.6290 - accuracy: 0.4 - ETA: 16:26 - loss: 1.6286 - accuracy: - ETA: 16:26 - loss: 1.6278 - accuracy: 0 - ETA: 16:26 - loss: 1.6272 - accuracy: 0.432 - ETA: 16:26 - loss: 1.6270 - accura - E - ETA: 16:23 -  - ETA: 16:21 - loss - ETA: 16:20 - loss: 1. - ETA: 16:19 - loss: 1.6153 - accuracy:  - ETA: 16:19 - loss: 1.6150 - accura - ETA: 16:18 - loss: 1.6139 - accuracy: 0.4 - ETA: 16:18 - loss: 1.6137 - accuracy: 0 - ETA: 16:18 - loss: 1.6134 - accuracy: 0.435 - ETA: 16:18 - loss: 1.613 - ETA: 16:16 - loss: 1.6122 - accuracy: 0.43 - ETA: 16:16 - loss: 1.6118 - accu - ETA: 16:16 - loss: 1.6110 - accuracy: 0.435 - ETA: 16:16 - loss: 1.6109 - accuracy: 0.4 - ETA: 16:15 - loss: 1.6107 - accuracy: 0.43 - ETA: 16:15 - loss: 1.6106 - accuracy: - ETA: 16:15 - loss: 1.6100 - accuracy: 0 - ETA: 16:15 - loss: 1.6094 - accuracy: 0.436 - ETA: 16:15 - loss:  - ETA: 16:13 - - ETA: 16:12 - loss: 1.6052 - accuracy: 0.43 - ETA: 16:11 - loss: 1.6049 - accuracy: 0.43 - ETA: 16:11 - loss: 1.6046 - accuracy: 0 - ETA: 16:11 - loss: 1.6046 - accuracy: 0.436 - ETA: 16:11 - loss: 1.6047 - a - ETA: 16:10 - loss: 1.6033 - accuracy: 0.43 - ETA: 16:10 - loss: 1.6032  - ETA: 16:09 - loss: 1.6016 - accuracy: 0.437 - ETA: 16:09 - loss: 1.6015 - accuracy: 0.437 - ETA: 16:09 - loss: 1.6014 - accuracy - ETA: 16:09 - loss: 1.6012 - accuracy: 0.43 - ETA: 16:09 - loss: 1.6010 - accuracy: - ETA: 16:08 - loss: 1.6004 - accurac - ETA: 16:08 - loss: 1.5998 - accuracy:  - ETA: 16:07 - loss: 1.5996 - accuracy: 0.43 - ETA: 16:07 - loss: 1.5995 - accuracy: 0.43 - ETA: 16:07 - loss: 1.5996 - accuracy: 0.4 - ETA: 16:07 - loss: 1.5993 - accuracy: 0.43 - ETA: 16:07 - loss: 1.5991 - accuracy: 0.4 - ETA: 16:07 - loss: 1.5990 - acc - ETA: 16:06 - loss: 1.597 - ETA: 16:05 - loss: 1.5960 - - ETA: 16:04 - loss: 1.5949 - accuracy: 0.437 - ETA: 16:04 - loss: 1.5948 - accuracy:  - ETA: 16:04 - loss: 1.5942 - ac - ETA: 16:03 - loss: 1.5936 - accuracy: 0.4 - ETA: 16:03 - loss: 1.5933 - accuracy: 0.4 - ETA: 16:02 - loss: 1.5930 - accuracy: - ETA: 16:02 - lo - ETA: 16:00 - loss: 1.5904 - accuracy: 0 - ETA: 16:00 - loss: 1.5902 - accuracy: 0.438 - ETA: 16:00 - loss: 1.5902 - accuracy: 0 - ETA: 16:00 - loss: 1.5901 - accuracy: 0.43 - ETA: 16:00 - loss: 1.5899 - accuracy: 0.4 - ETA: 15:59 - loss: 1.5897 - a - ETA: 15:59 - loss: 1.5885 - accuracy: - ETA: 15:58 - loss: 1.5880 - accuracy - ETA: 15:58 - loss: 1.5877 - accuracy:  - ETA: 15:57 - loss: 1.5872 - accuracy: 0.4 - ETA: 15:57 - loss: 1.5870 - accuracy: 0. - ETA: 15:57 - loss: 1.5869 - acc - ETA: 15:56 - loss: 1.5862 - accuracy - ETA: 15:56 - loss: 1.5857 - accur - ETA: 15:55 - loss: 1.5851 - accur - ETA: 15:54 - loss: 1.5844 - accuracy:  - ETA: 15:54 - loss: 1.5840 - accuracy: 0. - ETA: 15:54 - loss: 1.5838 - accuracy: 0.43 - ETA: 15:54 - loss: 1.5837 - accuracy: 0.4 - ETA: 15:54 - loss: 1.5837 - a - ETA: 15:53 - loss: 1.5826 - accuracy - ETA: 15:53 - loss: 1.5820 -  - ETA: 15:52 - loss: 1.5809 - accuracy: 0.437 - ETA: 15:49 - loss: 1.5784 - accuracy: 0. - ETA: 15:49 - loss: 1.5782 - accuracy: 0 - ETA: 15:48 - loss: 1.5778 - acc - ETA: 15:48 - loss: 1.5772 - accuracy - ETA: 15:47 - loss: 1.5768 - accur - ETA: 15:47 - loss: 1.5761  - ETA: 15:46  - ETA: 15:44 - loss: 1.5731 - acc - ETA: 15:43 - loss: 1.57 - ETA: 15:42 - loss: 1.5712 - ac - ETA: 15:41 - loss: 1.5706 - accuracy: 0.4 - ETA:  - ETA: 15:38 - loss: 1.5683 - ETA: 15:37 - loss: 1.5674 - accuracy: 0 - ETA: 15:37 - loss: 1.5674 - accura - ETA: 15:36 - loss: 1.5668 - accuracy: 0.43 - ETA: 15:36 - loss: 1.5667 - accuracy: 0.43 - ETA: 15:36 - loss: 1.5666 - accuracy: 0.4 - ETA: 15:36 - loss: 1.5665 - accuracy: 0. - ETA: 15:36 - loss: 1.5663 - accuracy: 0 - ETA: 15:35 - loss: 1.5660 - accuracy: 0.43 - ETA: 15:35 - loss: 1.5659 - accuracy:  - ETA: 15:35 - loss: 1.5654 - accuracy: 0.4 - ETA: 15:35 - loss: 1.56 - ETA: 15:33 - loss: 1.5640 - - ETA: 15:32 - loss: 1.5633 - accuracy: 0 - ETA: 15:32 - loss: 1.5629 - accuracy: 0.439 - ETA: 15:32 -  - ETA: 15:30 - loss: 1.5612 - a - ETA: 15:29 - loss: 1.5603 - accuracy - ETA: 15:29 - loss: 1.5600 - accuracy: 0. - ETA: 15:29 - loss: 1.5598 - accuracy: 0.441 - ETA: 15:29 - loss: 1.5597 - accur - ETA: 15:28 - loss: 1.5593 -  - ETA: 15:27 - loss: 1.5581 - accur - ETA: 15:26 - loss: 1.5577 - ETA: 15:25 - loss: 1.5565 - accuracy: 0.443 - ETA: 15:25 - loss: 1.5563 - accu - ETA: 15:24 - loss: 1.5557 - accuracy: - ETA: 15:24 - loss: 1.5555 - accuracy: 0.444 - ETA: 15:24 - loss: 1.5554 - accuracy: 0.44 - ETA: 15:24 - loss: 1.5553 - accuracy: 0.4 - ETA: 15:23 - loss: 1.5551 - accuracy: 0.4 - ETA: 15:23 - loss: 1.5548 - accuracy: 0.444 - ETA: 15:23 - loss: 1.5548 - accuracy - ETA: 15:23 - loss: 1.5543 - accura - ETA: 15:22 - loss: 1.553 - ETA: 15:21 - loss: 1.5527 - accuracy: 0.44 - ETA: 15:21 - loss: 1.552 - ETA: 15:20 - loss: 1.5513 - accuracy: 0. - ETA: 15:19 -   - ETA: 15:15 - loss: 1.5465 - accura - ETA: 15:14 - loss: 1.5460 - accura - ETA: 15:13 - loss: 1.5456 - accuracy:  - ETA: 15:13 - loss: 1.5451 - accuracy: 0.45 - ETA: 15:13 - loss: 1.5449 - accuracy: 0.4 - ETA: 15:13 - loss: 1.5447 - accu - ETA: 15:12 - loss: 1.5441 - accuracy: 0.452 - ETA: 15:12 - loss: 1 - ETA: 15:11 - loss:  - ETA: 15:09 - loss: 1.5417 - accuracy: 0.454 - ETA: 15:09 - loss: 1 - ETA: 15:07 - loss: 1.5 - ETA: 15:06 - loss: 1.5391 - accurac - ETA: 15:05 - loss: 1.5386 - accuracy: 0 - ETA: 15:05 - loss: 1.5384 - accuracy: 0. - ETA: 15:05 - loss: 1.538 - ETA: 15:04 - loss: 1.5374 - ac - ETA: 15:03 - loss: 1.5366 - acc - ETA: 15:02 - loss: 1.5358 - accuracy - ETA: 15:02 - loss: 1.5354 - accuracy: 0.459 - ETA: 15:01 - loss: 1.5352 - accuracy: 0.459 - ETA: 15:01 - loss: 1.5352 - accuracy: 0.45 - ETA: 15:01 - loss: 1.5350 - accuracy: 0.459 - ETA: 15:01 - loss: 1.5350 - accuracy: - ETA: 15:01 - loss: 1.5348 - accuracy: - ETA: 15:00 - loss: 1.5344 - accuracy: 0 - ETA: 15:00 - loss: 1.5341 - accuracy: 0 - ETA: 15:00 - loss: 1.5340 - accuracy:  - ETA: 14:59 - loss: 1.5337 - accuracy: - ETA: 14:59 - loss: 1.5334 - accuracy: 0.460 - ETA: 14:59 - loss: 1.5334 - accuracy: 0.4 - ETA: 14:59 - loss: 1.5333 - accuracy: 0.46 - ETA: 14:59 - loss: 1.5331 - accuracy: 0.46 - ETA: 14:59 - loss: 1.5331 - accuracy: 0.460 - ETA: 14:58 - loss: 1.5330 - accuracy - ETA: 14:58 - loss: 1.5326 - accura - ETA: 14:57 - loss: 1.53 - ETA: 14:56 - loss: 1.5312 - accuracy: 0.4 - ETA: 14:56 - loss: 1.5311 - a - ETA: 14:55 - loss: 1.5305 - accuracy: 0 - ETA: 14:55 - loss: 1.5303 -  - ETA: 14:54 - loss: 1.5296 - accuracy: - ETA: 14:53 - loss: 1.5293 -  - ETA: 14:52 - loss: 1.5284 - accuracy: 0.4 - ETA: 14:52 - loss: 1.5282 - accuracy:  - ETA: 14:52 - loss: 1.5279 - acc - ETA: 14:51 - loss: 1.5273 - accuracy:  - ETA: 14:51 - loss: 1.5269 - accuracy: 0.464 - ETA: 14:51 - los - ETA: 14:49 - loss: 1.5258 - accuracy: 0. - ETA: 14:49 - loss: 1.5256 - - ETA: 14:48 - loss: 1.5251 - accuracy: 0.4 - ETA: 14:48 - loss: 1.5249 - accuracy: 0. - ETA: 14:47 - loss: 1.5 - ETA: 14:46 - loss: 1.5234 -  - ETA: 14:45 - loss: 1.5226 - accuracy: 0.467 - ETA: 14:45 - loss: 1.5225 - accurac - ETA: 14:44 - loss: 1.5221 - accuracy:  - ETA: 14:44 - loss: 1.5217 - a - ETA: 14:43 - loss: 1.5209  - ETA: 14:42 - loss: 1.5201 - accura - ETA: 14:41 - loss: 1.5198 - accuracy:  - ETA: 14:41 - loss: 1.5196 - accurac - ETA: 14:41 - loss: 1.5192 - accuracy:  - ETA: 14:40 - loss: 1.5189 - accuracy - ETA: 14:40 - loss:  - ETA: 14:38 - loss: 1.5173 - accuracy: 0.471 - ETA: 14:38 - loss: 1.5173 - accuracy: 0.4711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 2266/16000 [===>..........................] - ETA: 14:38 - loss: 1.5172 - accuracy: 0.4712"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2730/16000 [====>.........................] - ETA: 14:10 - loss: 1.4961 - accuracy: 0.4835- ETA: 14:38 - loss: 1.516 - ETA: 14:37 - loss: 1.5158 - accur - ETA: 14:36 - loss: 1.5152 - accuracy: 0.472 - ETA: 14:36 - loss: 1.5152 - ac - ETA: 14:35 - loss: 1.5146 - accuracy: 0.4 - ETA: 14:35 - loss: 1.5145 - accuracy: 0.4 - ETA: 14:35 - loss: 1.5144 - accura - ETA: 14:34 - loss: 1.5139 - acc - ETA: 14:33 - loss: 1.5134 - accur - ETA: 14:33 - loss: 1.5129 - - ETA: 14:31 - loss: 1. - ETA: 14:30 - loss: 1.5111 - accuracy: 0.4 - ETA: 14:30 - loss: 1.5109 - accuracy: 0.475 - ETA: 14:30 - loss: 1.5108 - accuracy: 0. - ETA: 14:30 - loss: 1.5106 - accura - ETA: 14:29 -  - ETA: 14:27 - loss: 1.5087 - accu - ETA: 14:26 - loss: 1.5084 - accuracy - ETA: 14:26 - loss: 1.5080 - accuracy: 0 - ETA: 14:26 - loss: 1.5079 - accuracy: 0 - ETA: 14:25 - loss: 1. - ETA: 14:24 - loss: 1.5066 - accuracy: 0.4 - ETA: 14:24 - loss - ETA: 14:22 - loss: 1.5053 - - ETA: 14:21 - loss: 1.5045 - accuracy:  - ETA: 14:21 -  - ETA: 14:19 - loss: 1.5027 - accu - ETA: 14:18 - loss: 1.5022 - accuracy: - ETA: 14:17 - loss: 1.5019 - ac - ETA: 14:17 - loss: 1.5012 - accuracy:  - ETA: 14:16 - los - ETA: 14:15 - loss: 1.4996 - accuracy - ETA: 14:14 - loss: 1.4993 - accuracy: 0. - ETA: 14:14 - loss: 1.4991 - accuracy: 0.48 - ETA: 14:14 - loss: 1.4990 - accuracy: 0 - ETA: 14:13 - loss: 1.4988 - accuracy: - ETA: 14:13 - loss: 1.4985 - accuracy:  - ETA: 14:13 - loss: 1.4983 - ac - ETA: 14:12 - loss: 1.4978 - accuracy: 0.482 - ETA: 14:12 - loss: 1.4977 - accuracy: 0.48 - ETA: 14:11 - loss: 1.4976 - accuracy: 0 - ETA: 14:11 - loss: 1.4975 - accuracy: 0.48 - ETA: 14:11 - loss: 1.4974 - accuracy: 0.482 - ETA: 14:11 - loss: 1.4974 - accuracy: 0. - ETA: 14:11 - loss: 1.4971 - accuracy: 0.48 - ETA: 14:11 - loss: 1.4970 - accuracy: 0 - ETA: 14:10 - loss: 1.4967 - accur"
     ]
    }
   ],
   "source": [
    "print(\"Traning Model...\")\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4da2bc289e1d27d5225f68cb33352347e737c8a4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
